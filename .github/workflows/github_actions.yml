name: AQI Prediction Pipeline

on:
  schedule:
    - cron: '0 */1 * * *'   # Fetch data every 1 hours
    - cron: '0 6 * * *'     # Daily model training at 6 AM UTC
    - cron: '0 */6 * * *'   # Inference every 6 hours
  workflow_dispatch:        # Manual trigger enabled
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.9'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      should-fetch: ${{ steps.check.outputs.data_fetch }}
      should-train: ${{ steps.check.outputs.training }}
      should-infer: ${{ steps.check.outputs.inference }}
    steps:
      - id: check
        name: Determine tasks to run
        run: |
          event="${{ github.event_name }}"
          schedule="${{ github.event.schedule }}"
          
          # Determine if data fetching should run
          if [[ "$event" == "schedule" || "$event" == "workflow_dispatch" || "$event" == "push" ]]; then
            echo "data_fetch=true" >> $GITHUB_OUTPUT
          else
            echo "data_fetch=false" >> $GITHUB_OUTPUT
          fi
          
          # Determine if training should run (daily at 6 AM or manual/push)
          if [[ ("$event" == "schedule" && "$schedule" == "0 6 * * *") || "$event" == "workflow_dispatch" || "$event" == "push" ]]; then
            echo "training=true" >> $GITHUB_OUTPUT
          else
            echo "training=false" >> $GITHUB_OUTPUT
          fi
          
          # Determine if inference should run
          if [[ "$event" == "schedule" || "$event" == "workflow_dispatch" || "$event" == "push" ]]; then
            echo "inference=true" >> $GITHUB_OUTPUT
          else
            echo "inference=false" >> $GITHUB_OUTPUT
          fi

  data-fetching:
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should-fetch != 'false'
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          lfs: true
      - name: Setup Git LFS
        run: |
          git lfs install
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          lfs: true
      - name: Setup Git LFS
        run: |
          git lfs install
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          lfs: true
      - name: Setup Git LFS
        run: |
          git lfs install
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
      - name: Python setup
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
          fi
      - name: Set PYTHONPATH
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
      - name: Fetch Live AQI & Weather
        run: python data_fetching/fetch_aqi_data.py
        env:
          AQI_API_KEY: ${{ secrets.AQI_API_KEY }}
          WEATHER_API_KEY: ${{ secrets.WEATHER_API_KEY }}
      - name: Feature Engineering
        run: python feature_engineering/compute_features.py
      - name: Compress large files
        run: |
          # Compress large feature files
          find data/features -name "*.csv" -size +50M -exec gzip {} \;
          # Remove original large CSV files to avoid conflicts
          find data/features -name "*.csv" -size +50M -delete || true
      - name: Commit & Push Updated Data
        run: |
          # Pull latest changes first to avoid conflicts
          git pull origin ${{ github.ref }} --rebase || true
          
          # Add files (LFS will handle large files automatically)
          git add data/raw/ || true
          git add data/features/ || true
          git add data/processed/ || true
          
          if [[ -n "$(git status --porcelain)" ]]; then
            git commit -m "ðŸ”„ Update data: $(date -u '+%Y-%m-%d %H:%M UTC')" || exit 0
            git push origin ${{ github.ref }}
          fi

  inference:
    runs-on: ubuntu-latest
    needs: [setup, data-fetching]
    if: needs.setup.outputs.should-infer != 'false'
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      - name: Python setup
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
          fi
      - name: Set PYTHONPATH
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
      - name: Run Inference Pipeline
        run: python pipelines/inference_pipeline.py
        env:
          AQI_API_KEY: ${{ secrets.AQI_API_KEY }}
          WEATHER_API_KEY: ${{ secrets.WEATHER_API_KEY }}
      - name: Commit & Push Predictions
        run: |
          # Pull latest changes first to avoid conflicts
          git pull origin ${{ github.ref }} --rebase || true
          
          # Add prediction files (LFS will handle large files automatically)
          git add data/predictions/ || true
          
          if [[ -n "$(git status --porcelain)" ]]; then
            git commit -m "ðŸ”® Update predictions: $(date -u '+%Y-%m-%d %H:%M UTC')" || exit 0
            git push origin ${{ github.ref }}
          fi

  model-training:
    runs-on: ubuntu-latest
    needs: [setup, data-fetching]
    if: needs.setup.outputs.should-train != 'false'
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
      - name: Python setup
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
          fi
      - name: Set PYTHONPATH
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
      - name: Train Models
        run: python model_training/train_model.py
      - name: Generate SHAP Explanations
        run: python explainability/shap_explain.py
      - name: Commit & Push Model Updates
        run: |
          # Pull latest changes first to avoid conflicts
          git pull origin ${{ github.ref }} --rebase || true
          
          # Add model and explainability files (LFS will handle large files automatically)
          git add models/ || true
          git add explainability/ || true
          
          if [[ -n "$(git status --porcelain)" ]]; then
            git commit -m "ðŸ¤– Update models & SHAP: $(date -u '+%Y-%m-%d %H:%M UTC')" || exit 0
            git push origin ${{ github.ref }}
          fi

  deploy-dashboard:
    runs-on: ubuntu-latest
    needs: [data-fetching, inference, model-training]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - name: Python setup
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
          fi
      - name: Test Dashboard Import
        run: |
          python -c "
          import sys
          sys.path.append('.')
          try:
              import dashboard.app
              print('âœ“ Dashboard imports successfully')
          except Exception as e:
              print('âœ— Dashboard import failed:', e)
              exit(1)
          "
      - name: Mark Deployment Ready
        run: echo "Dashboard ready for deployment" > deployment-ready.txt
      - name: Upload Deployment Artifact
        uses: actions/upload-artifact@v4
        with:
          name: dashboard-deployment
          path: deployment-ready.txt

  notify:
    runs-on: ubuntu-latest
    needs: [data-fetching, inference, model-training, deploy-dashboard]
    if: always()
    steps:
      - name: Workflow Summary
        run: |
          echo "## ðŸš€ AQI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Data Fetch: ${{ needs.data-fetching.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Inference: ${{ needs.inference.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Model Training: ${{ needs.model-training.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Dashboard Deploy: ${{ needs.deploy-dashboard.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Timestamp: $(date -u '+%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
