name: AQI Prediction Pipeline

on:
  schedule:
    - cron: '0 */1 * * *'
    - cron: '0 6 * * *'
    - cron: '0 */6 * * *'
  workflow_dispatch:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

env:
  PYTHON_VERSION: '3.9'

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      should-fetch: ${{ steps.check.outputs.data_fetch }}
      should-train: ${{ steps.check.outputs.training }}
      should-infer: ${{ steps.check.outputs.inference }}
    steps:
      - id: check
        run: |
          event="${{ github.event_name }}"
          schedule="${{ github.event.schedule }}"
          [[ "$event" == "schedule" || "$event" == "workflow_dispatch" || "$event" == "push" ]] && echo "data_fetch=true" >> $GITHUB_OUTPUT || echo "data_fetch=false" >> $GITHUB_OUTPUT
          [[ ("$event" == "schedule" && "$schedule" == "0 6 * * *") || "$event" == "workflow_dispatch" || "$event" == "push" ]] && echo "training=true" >> $GITHUB_OUTPUT || echo "training=false" >> $GITHUB_OUTPUT
          [[ "$event" == "schedule" || "$event" == "workflow_dispatch" || "$event" == "push" ]] && echo "inference=true" >> $GITHUB_OUTPUT || echo "inference=false" >> $GITHUB_OUTPUT

  data-fetching:
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should-fetch != 'false'
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          lfs: false
      
      # - name: Setup Git LFS
      #   run: |
      #     git lfs install
      #     git lfs pull || echo "âš ï¸ LFS pull had issues, continuing..."
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt || pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
      
      - name: Set Python path
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
      
      - name: Fetch new data
        run: python data_fetching/fetch_aqi_data.py
        env:
          AQI_API_KEY: ${{ secrets.AQI_API_KEY }}
          WEATHER_API_KEY: ${{ secrets.WEATHER_API_KEY }}
      
      - name: Compute features
        run: python feature_engineering/compute_features.py
      
      - name: Implement data retention policy
        run: |
          echo "ðŸ—‚ï¸ Implementing data retention policy..."
          cd data/features
          
          # List all feature files by date (newest first), keep only last 3
          ls -t features_*.csv* 2>/dev/null | head -3 > keep_files.txt || touch keep_files.txt
          
          # Remove old files from LFS tracking if more than 3 exist
          file_count=$(ls -1 features_*.csv* 2>/dev/null | wc -l || echo "0")
          if [ "$file_count" -gt 3 ]; then
            echo "Found $file_count feature files, cleaning up older ones..."
            
            for file in $(ls -t features_*.csv* 2>/dev/null | tail -n +4); do
              echo "Removing $file from LFS tracking..."
              git lfs untrack "data/features/$file" || true
              
              # Compress if not already compressed and larger than 10MB
              if [[ "$file" != *.gz ]] && [[ $(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null) -gt 10485760 ]]; then
                echo "Compressing $file..."
                gzip -9 "$file"
                file="${file}.gz"
              fi
            done
          fi
          
          # Clean up temp file
          rm -f keep_files.txt
          cd ../..
          
          echo "âœ… Data retention policy applied"
      
      - name: Compress large files
        run: |
          echo "ðŸ“¦ Compressing large data files..."
          find data/features -name "*.csv" -size +10M -exec gzip -9 {} \;
          echo "âœ… Compression complete"
      
      - name: Commit data updates
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "ðŸ”„ Update data: $(date -u '+%Y-%m-%d %H:%M UTC')" || echo "Nothing to commit"
          git pull --rebase origin main || echo "Nothing to rebase"
          git push origin main || echo "Push failed"

  inference:
    runs-on: ubuntu-latest
    needs: [setup, data-fetching]
    if: needs.setup.outputs.should-infer != 'false'
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          lfs: false
      
      # - name: Setup Git LFS
      #   run: |
      #     git lfs install
      #     git lfs pull || echo "âš ï¸ LFS pull had issues, continuing..."
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt || pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
      
      - name: Set Python path
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
      
      - name: Run inference
        run: python pipelines/inference_pipeline.py
        env:
          AQI_API_KEY: ${{ secrets.AQI_API_KEY }}
          WEATHER_API_KEY: ${{ secrets.WEATHER_API_KEY }}
      
      - name: Commit predictions
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "ðŸ”® Update predictions: $(date -u '+%Y-%m-%d %H:%M UTC')" || echo "Nothing to commit"
          git pull --rebase origin main || echo "Nothing to rebase"
          git push origin main || echo "Push failed"

  model-training:
    runs-on: ubuntu-latest
    needs: [setup, data-fetching]
    if: needs.setup.outputs.should-train != 'false'
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0
          lfs: false
      
      # - name: Setup Git LFS
      #   run: |
      #     git lfs install
      #     git lfs pull || echo "âš ï¸ LFS pull had issues, continuing..."
      
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt || pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
      
      - name: Set Python path
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
      
      - name: Train models
        run: python model_training/train_model.py
      
      - name: Generate SHAP explanations
        run: python explainability/shap_explain.py
      
      - name: Commit model updates
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "ðŸ¤– Update models & SHAP: $(date -u '+%Y-%m-%d %H:%M UTC')" || echo "Nothing to commit"
          git pull --rebase origin main || echo "Nothing to rebase"
          git push origin main || echo "Push failed"

  deploy-dashboard:
    runs-on: ubuntu-latest
    needs: [data-fetching, inference, model-training]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - run: |
          pip install --upgrade pip
          pip install -r requirements.txt || pip install requests pandas numpy scikit-learn xgboost joblib shap streamlit plotly
      - run: |
          python -c "
          import sys
          sys.path.append('.')
          try:
              import dashboard.app
              print('âœ“ Dashboard imports successfully')
          except Exception as e:
              print('âœ— Dashboard import failed:', e)
              exit(1)
          "
      - run: echo "Dashboard ready for deployment" > deployment-ready.txt
      - uses: actions/upload-artifact@v4
        with:
          name: dashboard-deployment
          path: deployment-ready.txt

  notify:
    runs-on: ubuntu-latest
    needs: [data-fetching, inference, model-training, deploy-dashboard]
    if: always()
    steps:
      - run: |
          echo "## ðŸš€ AQI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Data Fetch: ${{ needs.data-fetching.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Inference: ${{ needs.inference.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Model Training: ${{ needs.model-training.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Dashboard Deploy: ${{ needs.deploy-dashboard.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Timestamp: $(date -u '+%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
          